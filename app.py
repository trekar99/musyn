import gradio as gr
import torch
import numpy as np
from utils.audio_utils import get_audio, preprocess_audio
import config
from model.txt2img import ImageGenerator
from model.music2txt import MusicCapsGenerator

# Image Generation
generator = ImageGenerator()

# Prompt Generator
model = MusicCapsGenerator()

device = "cuda:0" if torch.cuda.is_available() else "cpu"

def captioning(audio, last_inf):
    audio_tensor = get_audio(audio) if isinstance(audio, str) else preprocess_audio(audio, 16000)
    if (audio_tensor.shape[1] == 160000):
      if device is not None:
          audio_tensor = audio_tensor.to(device)
      with torch.no_grad():
          output = model.generate(
              samples=audio_tensor,
              num_beams=5,
          )
      inference = ""
      number_of_chunks = range(audio_tensor.shape[0])
      for chunk, text in zip(number_of_chunks, output):
          inference += f"{text} \n \n"
    else:
      inference = last_inf

    return inference, audio

with gr.Blocks(title = config.title, css = config.css) as demo:
    gr.Markdown(config.web_title, elem_id="intro")
    gr.Markdown(config.description, elem_id="intro")
    #gr.LoginButton(size="md", icon=None)

    with gr.Tab("Live Audio"):
      with gr.Row():
          with gr.Column():
              # Audio pierde mucha informaci√≥n
              input_audio = gr.Audio(label="Input", sources="microphone")
              input_caption = gr.Textbox(label="Input Prompt")

              with gr.Accordion("Advanced options", open=False):
                    width = gr.Dropdown(config.width, label="Width")
                    height = gr.Dropdown(config.height, label="Height")

                    timer = gr.Timer(0)
                    gr.Button("Resume Generation", size="md").click(lambda: gr.Timer(active=True), None, timer)
                    gr.Button("Stop Generation", size="md").click(lambda: gr.Timer(active=False), None, timer)

              examples = gr.Examples(config.examples, input_caption)

          with gr.Column():
              output_caption = gr.Textbox(label="Caption generated by LP-MusicCaps Transfer Model")
              output_audio = gr.Audio()

          with gr.Column():
              output_img = gr.Image()

      input_audio.stream(captioning,
                        [input_audio, output_caption],
                        [output_caption, output_audio],
                        time_limit=0, stream_every=10, concurrency_limit=30
                        )
      
      timer.tick(generator.image_generator, [input_caption, output_caption, width, height], output_img)

    with gr.Tab("File Audio"):
      with gr.Row():
        with gr.Column():
          input_audio2 = gr.Audio(label="Input", type="filepath")
          input_caption2 = gr.Textbox(label="Input Prompt")

        with gr.Column():
          output_caption2 = gr.Textbox(label="Caption generated by LP-MusicCaps Transfer Model")
          output_audio2 = gr.Audio()
          
        with gr.Column():
          output_img2 = gr.Image(label="Output")

      with gr.Row():
        btn = gr.Button("Run")
        btn.click(fn=captioning, 
                  inputs=[input_audio2, output_caption2], 
                  outputs=[output_caption2, output_audio2]).then(generator.image_generator, [input_caption2, output_caption2], output_img2)

if __name__ == "__main__":
    try:
      torch.cuda.set_device(device)
      generator.pipeline.to(device)
      generator.vae.to(device)
      model.to(device)
    except:
      print("ERROR: Run it in a CUDA device.")
      exit()

    # Warm-up model
    audio_tensor = torch.zeros(1, 160000)
    audio_tensor = audio_tensor.to(device)
    with torch.no_grad():
        output = model.generate(
            samples=audio_tensor,
            num_beams=5,
        )
    
    demo.launch(share=True, debug=True, favicon_path="./favicon.png", inline=False, inbrowser=True)
