import gradio as gr
import torch
import numpy as np
from model.bart import BartCaptionModel
from utils.audio_utils import get_audio, preprocess_audio
import config

import librosa 

# Image Generation
from model.txt2img import ImageGenerator
generator = ImageGenerator()

# Prompt Generator
device = "cuda:0" if torch.cuda.is_available() else "cpu"

model = BartCaptionModel(max_length = 128)
pretrained_object = torch.load('model/models/lpmusiccaps.pth', map_location='cpu')
state_dict = pretrained_object['state_dict']
model.load_state_dict(state_dict)
if torch.cuda.is_available():
    torch.cuda.set_device(device)
    generator.pipeline.to(device)
model = model.cuda(device)
model.eval()

def captioning(audio, prompt):
    audio_tensor = get_audio(audio) if isinstance(audio, str) else preprocess_audio(audio)

    if device is not None:
        audio_tensor = audio_tensor.to(device)
    with torch.no_grad():
        output = model.generate(
            samples=audio_tensor,
            num_beams=5,
        )
    inference = ""
    number_of_chunks = range(audio_tensor.shape[0])
    for chunk, text in zip(number_of_chunks, output):
        inference += f"{text} \n \n"
    return inference, generator.image_generator(prompt + inference)


with gr.Blocks() as demo:
    gr.Markdown(
              f'''# {config.title}''',
              elem_id="intro",
          )
    gr.Markdown(config.description)
    with gr.Tab("Live Audio"):
      with gr.Row():
          with gr.Column():
              # Audio pierde mucha informaci√≥n
              input_img = gr.Audio(label="Input", sources="microphone", waveform_options=gr.WaveformOptions(
                  waveform_color="#01C6FF",
                  waveform_progress_color="#0066B4",
                  skip_length=2,
                  show_controls=False,
              ),)
              input_cpt = gr.Textbox(label="Input Prompt")

          with gr.Column():
              output_cpt = gr.Textbox(label="Caption generated by LP-MusicCaps Transfer Model")
              output_img = gr.Image(label="Output")
              #output_audio = gr.Audio()
          input_img.stream(captioning, [input_img, input_cpt], [output_cpt, output_img], time_limit=0, stream_every=10, concurrency_limit=30)
    with gr.Tab("File Audio"):
      with gr.Row():
        with gr.Column():
          input_img = gr.Audio(label="Input", type="filepath")
          input_cpt = gr.Textbox(label="Input Prompt")
        with gr.Column():
          output_cpt = gr.Textbox(label="Caption generated by LP-MusicCaps Transfer Model")
          output_img = gr.Image(label="Output")
      with gr.Row():        
        btn = gr.Button("Run")
        btn.click(fn=captioning, inputs=[input_img, input_cpt], outputs=[output_cpt, output_img])

if __name__ == "__main__":

    demo.launch(share=True)