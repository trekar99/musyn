import gradio as gr
import torch
import numpy as np
from model.bart import BartCaptionModel
from utils.audio_utils import load_audio, STR_CH_FIRST

# Image Generation
from diffusers import StableDiffusionXLPipeline
from huggingface_hub import snapshot_download

# TODO download
#from diffusers import AutoencoderKL
'''vae = AutoencoderKL.from_pretrained(
  'madebyollin/sdxl-vae-fp16-fix',
  use_safetensors=True,
  torch_dtype=torch.float16,
).to('cuda')'''

# TODO download
my_local_config_path = snapshot_download(
    repo_id="stabilityai/sdxl-turbo",
    allow_patterns=["*.json", "**/*.json", "*.txt", "**/*.txt"],
    local_dir="assets/models/config"
)
pipeline = StableDiffusionXLPipeline.from_single_file("assets/models/sdxlturbo.safetensors",
                                                      config=my_local_config_path,
                                                      local_files_only=True,
                                                      revision="fp16",
                                                      variant="fp16",
                                                      torch_dtype=torch.float16)
#pipeline.to("cuda")

pipeline.set_progress_bar_config(disable=True)

def image_generator(prompt):
    return pipeline(prompt=prompt,
                num_inference_steps=1,
                guidance_scale=0.0
                ).images[0]

# Prompt Generator
device = "cuda:0" if torch.cuda.is_available() else "cpu"

#example_list = ['electronic.mp3', 'orchestra.wav']
model = BartCaptionModel(max_length = 128)
pretrained_object = torch.load('assets/models/lpmusiccaps.pth', map_location='cpu')
state_dict = pretrained_object['state_dict']
#state_dict['audio_encoder.positional_embedding'] = torch.tensor([[1.]])
model.load_state_dict(state_dict)
#print(torch.cuda.get_device_name())
if torch.cuda.is_available():
    torch.cuda.set_device(device)
    pipeline.to(device)
model = model.cuda(device)
model.eval()
# Print model's state_dict
#print("Model's state_dict:")
#for param_tensor in model.state_dict():
#    print(param_tensor, "\t", model.state_dict()[param_tensor].size())


def get_audio(audio_path, duration=10, target_sr=16000):
    n_samples = int(duration * target_sr)
    audio, sr = load_audio(
        path= audio_path,
        ch_format= STR_CH_FIRST,
        sample_rate= target_sr,
        downmix_to_mono= True,
    )
    if len(audio.shape) == 2:
        audio = audio.mean(0, False)  # to mono
    input_size = int(n_samples)
    if audio.shape[-1] < input_size:  # pad sequence
        pad = np.zeros(input_size)
        pad[: audio.shape[-1]] = audio
        audio = pad
    ceil = int(audio.shape[-1] // n_samples)
    audio = torch.from_numpy(np.stack(np.split(audio[:ceil * n_samples], ceil)).astype('float16'))# 32--> 16
    return audio

def captioning(audio_path):
    audio_tensor = get_audio(audio_path = audio_path)
    if device is not None:
        audio_tensor = audio_tensor.to(device)
    with torch.no_grad():
        output = model.generate(
            samples=audio_tensor,
            num_beams=5,
        )
    inference = ""
    number_of_chunks = range(audio_tensor.shape[0])
    for chunk, text in zip(number_of_chunks, output):
        inference += f"{text} \n \n"
    return inference, image_generator(inference)

title = "Musyn - Music Synesthesia in Real-Time"
description = """
<p style='text-align: center'> Musyn: Real-time Music-to-Image Co-creation System</p> 
<p style='text-align: center'> Germ√°n Puerto - 2025</p> 
"""
"""<p style='text-align: center'> <a href='#' target='_blank'>ArXiv</a> | <a href='https://github.com/seungheondoh/lp-music-caps' target='_blank'>Github</a> | <a href='https://github.com/seungheondoh/lp-music-caps' target='_blank'>LP-MusicCaps-Dataset</a> </p>
<p style='text-align: center'> To use it, simply upload your audio and click 'submit', or click one of the examples to load them. Read more at the links below. </p>
"""


demo = gr.Interface(fn=captioning,
                    inputs=gr.Audio(type="filepath"),
                    outputs=[
                        gr.Textbox(label="Caption generated by LP-MusicCaps Transfer Model"),
                        gr.Image()
                        ],
                    title=title,
                    description=description,
                    cache_examples=False
                    )
demo.launch(share=True)