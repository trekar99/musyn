import gradio as gr
import torch
import numpy as np
from model.bart import BartCaptionModel
from utils.audio_utils import get_audio, preprocess_audio
import config

import librosa 

# Image Generation
from model.txt2img import ImageGenerator
generator = ImageGenerator()

# Prompt Generator
device = "cuda:0" if torch.cuda.is_available() else "cpu"

model = BartCaptionModel(max_length = 128)
pretrained_object = torch.load('model/models/lpmusiccaps.pth', map_location='cpu')
state_dict = pretrained_object['state_dict']
model.load_state_dict(state_dict)
if torch.cuda.is_available():
    torch.cuda.set_device(device)
    generator.pipeline.to(device)
model = model.cuda(device)
model.eval()

chingadera = ""

async def captioning(audio, prompt):
    audio_tensor = get_audio(audio) if isinstance(audio, str) else preprocess_audio(audio)

    if device is not None:
        audio_tensor = audio_tensor.to(device)
    with torch.no_grad():
        output = model.generate(
            samples=audio_tensor,
            num_beams=5,
        )
    inference = ""
    number_of_chunks = range(audio_tensor.shape[0])
    for chunk, text in zip(number_of_chunks, output):
        inference += f"{text} \n \n"

    global chingadera
    chingadera = inference
    
    return inference, generator.image_generator(prompt + inference)


css = """
#container{
    margin: 0 auto;
    max-width: 80rem;
}
#intro{
    max-width: 100%;
    text-align: center;
    margin: 0 auto;
}"""

with gr.Blocks(title = "Musyn", css=css) as demo:
    gr.Markdown(
              f'''# {config.title}''',
              elem_id="intro",
          )
    gr.Markdown(config.description)
    with gr.Tab("Live Audio"):
      with gr.Row():
          with gr.Column():
              # Audio pierde mucha informaci√≥n
              input_audio = gr.Audio(label="Input", sources="microphone")
              input_caption = gr.Textbox(label="Input Prompt")
              with gr.Accordion("Advanced options", open=False):
                        seed = gr.Slider(
                            randomize=True,
                            minimum=0,
                            maximum=12013012031030,
                            label="Seed",
                            step=1,
                        )

          with gr.Column():
              output_cpt = gr.Textbox(label="Caption generated by LP-MusicCaps Transfer Model")
          with gr.Column():
              output_img = gr.Image(type="filepath")

      with gr.Row():
          examples = gr.Examples(
          examples=[
              ["Brasilian Beach, Rio de Janeiro"],
              ["Gnawa Village, Morocco"]
          ],
          inputs=[input_caption],
      )
      
      input_audio.stream(captioning, [input_audio, input_caption], [output_cpt, output_img], time_limit=0, stream_every=10, concurrency_limit=30)
      #seed.change(generator.image_generator, [input_caption, seed], output_img)

    with gr.Tab("File Audio"):
      with gr.Row():
        with gr.Column():
          input_audio = gr.Audio(label="Input", type="filepath")
          input_caption = gr.Textbox(label="Input Prompt")
        with gr.Column():
          output_cpt = gr.Textbox(label="Caption generated by LP-MusicCaps Transfer Model")
          output_img = gr.Image(label="Output")
      with gr.Row():        
        btn = gr.Button("Run")
        btn.click(fn=captioning, inputs=[input_audio, input_caption], outputs=[output_cpt, output_img])
      
    #input_cpt.change(generator.image_generator(str(input_cpt.value) + str(output_cpt.value)), [input_cpt, output_cpt], outputs=output_img)

if __name__ == "__main__":
    demo.launch(share=True, debug=True, favicon_path="/content/musyn/favicon.png", inline=False)
